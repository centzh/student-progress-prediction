{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4b297c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_3828\\3906806925.py:39: DtypeWarning: Columns (18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(challenge_path, file))\n",
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_3828\\3906806925.py:39: DtypeWarning: Columns (18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(challenge_path, file))\n",
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_3828\\3906806925.py:39: DtypeWarning: Columns (18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(challenge_path, file))\n",
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_3828\\3906806925.py:39: DtypeWarning: Columns (18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(challenge_path, file))\n",
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_3828\\3906806925.py:39: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(challenge_path, file))\n",
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_3828\\3906806925.py:39: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(challenge_path, file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Challenge: challenge-beginners-2018 ===\n",
      "\n",
      "Model: LogReg\n",
      "  Overall Accuracy: 0.946\n",
      "  Per Class Accuracy:\n",
      "    class0: 0.000\n",
      "    class1: 0.000\n",
      "    class2: 0.000\n",
      "\n",
      "Model: RF\n",
      "  Overall Accuracy: 0.942\n",
      "  Per Class Accuracy:\n",
      "    class0: 0.000\n",
      "    class1: 0.000\n",
      "    class2: 0.000\n",
      "\n",
      "Model: SVM\n",
      "  Overall Accuracy: 0.945\n",
      "  Per Class Accuracy:\n",
      "    class0: 0.000\n",
      "    class1: 0.000\n",
      "    class2: 0.000\n",
      "\n",
      "Model: DT\n",
      "  Overall Accuracy: 0.926\n",
      "  Per Class Accuracy:\n",
      "    class0: 0.000\n",
      "    class1: 0.000\n",
      "    class2: 0.000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Extract per-class accuracy using numeric keys directly from the report\n",
    "    per_class_acc = {\n",
    "        cls: report[str(cls)]['recall']  # Use string of the class number (e.g., '0', '1', '2')\n",
    "        for cls in np.unique(y_true)  # This ensures we use all class labels present in y_true\n",
    "    }\n",
    "    return overall_acc, per_class_acc\n",
    "\n",
    "root_dir = 'data-processed/Experiment 3 Data'\n",
    "results = {}\n",
    "\n",
    "for challenge in os.listdir(root_dir):\n",
    "    if challenge != \"challenge-beginners-2018\":\n",
    "        continue\n",
    "    challenge_path = os.path.join(root_dir, challenge)\n",
    "    if not os.path.isdir(challenge_path):\n",
    "        continue\n",
    "\n",
    "    challenge_results = {'LogReg': [], 'RF': [], 'SVM': [], 'DT': []}\n",
    "\n",
    "    for file in os.listdir(challenge_path):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(os.path.join(challenge_path, file))\n",
    "\n",
    "        if 'Outcome' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Assume the 'slide' columns are named like 'slide1', 'slide2', ..., 'slideN'\n",
    "        slide_cols = [col for col in df.columns if col.startswith('slide')]\n",
    "        X_raw = df[slide_cols]\n",
    "        y_raw = df['Outcome']\n",
    "\n",
    "        # Convert event slides (Zero/One) and problem slides (class0, class1, class2) to numeric\n",
    "        for col in X_raw.columns:\n",
    "            try:\n",
    "                X_raw[col] = pd.to_numeric(X_raw[col])\n",
    "                print(col)\n",
    "            except ValueError:\n",
    "                continue  # Leave as is for one-hot encoding later\n",
    "\n",
    "        # One-hot encode categorical columns for problem slides (class0, class1, class2)\n",
    "        X = pd.get_dummies(X_raw)\n",
    "\n",
    "        # Drop rows with any missing data\n",
    "        mask = X.notnull().all(axis=1) & y_raw.notnull()\n",
    "        X = X[mask]\n",
    "        y = y_raw[mask]\n",
    "\n",
    "        if X.shape[0] < 10:\n",
    "            print(f\"Skipping {file} in {challenge} (too few samples after cleanup).\")\n",
    "            continue\n",
    "\n",
    "        encoder = LabelEncoder()\n",
    "        y_enc = encoder.fit_transform(y)\n",
    "\n",
    "        if len(np.unique(y_enc)) < 2:\n",
    "            print(f\"Skipping {file} in {challenge} (only one class present).\")\n",
    "            continue\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        fold_metrics = {'LogReg': [], 'RF': [], 'SVM': [], 'DT': []}\n",
    "\n",
    "        for train_idx, test_idx in skf.split(X, y_enc):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y_enc[train_idx], y_enc[test_idx]\n",
    "\n",
    "            # Logistic Regression\n",
    "            logreg = LogisticRegression(max_iter=1000)\n",
    "            logreg.fit(X_train, y_train)\n",
    "            acc, per_class = evaluate_model(y_test, logreg.predict(X_test))\n",
    "            fold_metrics['LogReg'].append((acc, per_class))\n",
    "\n",
    "            # Random Forest\n",
    "            rf = RandomForestClassifier(n_estimators=100)\n",
    "            rf.fit(X_train, y_train)\n",
    "            acc, per_class = evaluate_model(y_test, rf.predict(X_test))\n",
    "            fold_metrics['RF'].append((acc, per_class))\n",
    "\n",
    "            # SVM\n",
    "            svm = SVC()\n",
    "            svm.fit(X_train, y_train)\n",
    "            acc, per_class = evaluate_model(y_test, svm.predict(X_test))\n",
    "            fold_metrics['SVM'].append((acc, per_class))\n",
    "\n",
    "            # Decision Tree (default)\n",
    "            dt = DecisionTreeClassifier()\n",
    "            dt.fit(X_train, y_train)\n",
    "            acc, per_class = evaluate_model(y_test, dt.predict(X_test))\n",
    "            fold_metrics['DT'].append((acc, per_class))\n",
    "\n",
    "        for model in fold_metrics:\n",
    "            accs = [m[0] for m in fold_metrics[model]]\n",
    "            per_class_all = [m[1] for m in fold_metrics[model]]\n",
    "            avg_acc = np.mean(accs)\n",
    "\n",
    "            # Calculate per-class accuracy for class0, class1, class2 only\n",
    "            avg_class_acc = {\n",
    "                k: np.mean([d.get(k, 0) for d in per_class_all]) for k in [1, 2, 3]\n",
    "            }\n",
    "\n",
    "            challenge_results[model].append((avg_acc, avg_class_acc))\n",
    "\n",
    "    # Average across all files in the challenge\n",
    "    results[challenge] = {}\n",
    "    for model in challenge_results:\n",
    "        accs = [r[0] for r in challenge_results[model]]\n",
    "        per_class_all = [r[1] for r in challenge_results[model]]\n",
    "        avg_acc = np.mean(accs) if accs else 0\n",
    "\n",
    "        avg_class_acc = {\n",
    "            k: np.mean([d.get(k, 0) for d in per_class_all]) for k in ['class0', 'class1', 'class2']\n",
    "        }\n",
    "\n",
    "        results[challenge][model] = {\n",
    "            'Overall Accuracy': avg_acc,\n",
    "            'Per Class Accuracy': avg_class_acc\n",
    "        }\n",
    "\n",
    "# Print report\n",
    "for challenge, models in results.items():\n",
    "    print(f\"\\n=== Challenge: {challenge} ===\")\n",
    "    for model, metrics in models.items():\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(f\"  Overall Accuracy: {metrics['Overall Accuracy']:.3f}\")\n",
    "        print(\"  Per Class Accuracy:\")\n",
    "        for cls, acc in metrics['Per Class Accuracy'].items():\n",
    "            print(f\"    {cls}: {acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
